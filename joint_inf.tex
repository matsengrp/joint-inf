\documentclass{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Useful packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{showlabels}

\graphicspath{ {figures/} }

% Commands
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\alphabet}{\mathcal{A}}
\newcommand{\fullAlignment}{\mathbf{Y}}
\newcommand{\alignmentColumn}{\mathbf{y}}
\newcommand{\alignmentColumnRV}{Y}
\newcommand{\siteSplit}{\tilde{s}}
\newcommand{\siteSplitSet}{\mathcal{S}}
\newcommand{\fullAncestralStates}{\mathbf{H}}
\newcommand{\ancestralStateColumn}{\mathbf{h}}
\newcommand{\ancestralStateColumnRV}{H}
\newcommand{\ancestralSplit}{\tilde{h}}
\newcommand{\ancestralSplitSet}{\mathcal{H}}
\newcommand{\ancestralSplitPartition}{\eta}
\newcommand{\fullAncestralSplitPartitions}{\boldsymbol\eta}

\newcommand{\patternToSplit}{\psi}
\newcommand{\ancestralToSplit}{\xi}

\newcommand{\siteSplitRV}{\Psi}
\newcommand{\ancestralSplitRV}{\Xi}

\newcommand{\nCols}{n}
\newcommand{\nSiteRows}{m}
\newcommand{\nAncestralStateRows}{p}
\newcommand{\nSiteSplits}{q}
\newcommand{\nAncestralSplits}{r}

\newcommand{\shannonDivergence}{D}

\DeclareMathOperator*{\argmax}{argmax}

\allowdisplaybreaks

\title{Joint estimation of ancestral states and phylogenetic model parameters is inconsistent}
\author{Shaw, Matsen, Minin}

\begin{document}
\maketitle

\renewcommand{\arraystretch}{1.2} % because otherwise exponents get eaten by \hline


\section*{Introduction}

Classical maximum-likelihood (ML) estimation in phylogenetics operates by integrating out possible ancestral states at the internal nodes of the tree.
Recently, \cite{Sagulenko2017-jo} have suggested using an approximation to ML inference in which the likelihood is maximized jointly across model parameters and ancestral sequences.
This is attractive from a computational perspective: such joint ML inference can proceed according to an iterative procedure in which ML ancestral sequences are first inferred and then model parameters are optimized conditional on the ancestral sequences.
This latter conditional optimization is simpler and more computationally efficient than marginalizing out ancestral sequences and then performing optimization.

Existing consistency proofs for phylogenetics \cite{RoyChoudhury2015-ta} apply only to estimating model parameters when the ancestral sequences have been integrated out of the likelihood.
These proofs do not readily extend to include estimating ancestral states.
Moreover, examples of inconsistency arising from including a large number of additional parameters \cite{Neyman1948-tt} indicate that joint inference of trees and ancestral states may not enjoy good statistical properties.
Although \cite{Sagulenko2017-jo} explicitly warn that their approximation is for the case of branch lengths $\ll 1$ and limit themselves to optimizing branch lengths using this method, their work motivates understanding when exactly this approximation breaks down, and what happens when we optimize branch lengths and tree topology using this approximation.

%EM clarify that we aren't doing sequence-level simulation somewhere.
In this paper, we show that the joint inference of trees and ancestral sequences is not consistent in general.
To do so, we use a binary symmetric model and imagine data being generated on the four taxon ``Farris zone'' \cite{Siddall1998-hq} tree, and we construct bounds on the joint likelihood to demarcate a sizeable area of long branch lengths in which joint inference is guaranteed to give the wrong tree in the large-data limit.
We find similar areas where joint inference consistently overestimates interior branch lengths when the topology is known.

%EM Potential re-outline--
%
%EM 1. Describe setup with Fels and Farris trees, and how we are talking about the case of infinite data.
%EM 2. Describe fidelities
%EM 3. State theorems
%EM 4. Plots
%EM 5. Discussion, already below. This motivates a more thorough investigation under simulation.

\section{Phylogenetic maximum likelihood}

Set up mild notation needed to state likelihood

We consider the standard phylogenetic likelihood on unrooted trees \cite{Felsenstein2004}.
We assume the character alphabet $\alphabet=\{0,1\}$ and a uniform stationary distribution.
Let $\nSiteRows$ be the number of tips of the tree, and $\nAncestralStateRows = \nSiteRows-2$ the number of internal nodes.
We observe $\nCols$ samples of character data, i.e., an alignment with $\nCols$ columns, $\fullAlignment=[\alignmentColumn_1,\ldots,\alignmentColumn_\nCols]\in\alphabet^{\nSiteRows\times\nCols}$ distributed as the random variable $\alignmentColumnRV$.
The corresponding unobserved ancestral states are $\fullAncestralStates=[\ancestralStateColumn_1,\ldots,\ancestralStateColumn_\nCols]\in\alphabet^{\nAncestralStateRows\times\nCols}$ and distributed as $\ancestralStateColumnRV$.

\subsection{Phylogenetic parameters}

\subsubsection{Topology}

\begin{figure}
\centering
\begin{subfigure}{.45\linewidth}
\centering
\includegraphics[width=.95\textwidth]{farris_blank}
\caption[short]{Farris topology $\tau_1$}
\end{subfigure}
\begin{subfigure}{.45\linewidth}
\centering
\includegraphics[width=.95\textwidth]{felsenstein_blank}
\caption[short]{Felsenstein topology $\tau_2$}
\end{subfigure}
\caption{Two simple topologies}
\label{fig:farris-fels-top}
\end{figure}

Consider the fixed, binary four-tip tree $\tau_1$ in Fig.~\ref{fig:farris-fels-top}---this is commonly known as the ``Farris zone'' topology.

\subsubsection{Fidelities}

We parameterize the branches of these trees not with the standard notion of branch length in terms of number of substitutions per site, but with a parameter such that the probability of a substitution on a branch with parameter $\theta$ is $p_\theta = (1-\theta)/2$ while the probability of no substitution is $(1+\theta)/2$.
This parametrization has been called edge ``fidelity'' as it quantifies the fidelity of transmission of the ancestral state along an edge \cite{Matsen2007-jq}.
Fidelities have useful algebraic properties, such as the ability to write generating probabilities using the Hadamard transform (see either 8.6 of Semple and Steel \cite{Semple2003-em} or Appendix (TODO)).

\subsection{Classical maximum likelihood}

For a topology $\tau$ and branch lengths $t$, we make the usual assumption of independence between sites, obtaining the likelihood
\begin{equation}
\label{eq:full_likelihood}
%EM given that the H is on the lhs of the conditioning, does it make sense to have it on the rhs of the semicolon? I don't know the convention.
%das: I forgot that standard likelihoods are written with the parameters on the left.
%Since H is like a hidden variable, I went for an expectation-maximization notation, grouping them with the observations.
%It seems a little wonky, though, when we start having maximum likelihood ancestral states depending on \tau,t.
%Oh well...
L_\nCols(\tau, t; \fullAlignment,\fullAncestralStates) = \prod_{i=1}^{\nCols} \ P(\alignmentColumnRV=\alignmentColumn_i, \ancestralStateColumnRV=\ancestralStateColumn_i \mid \tau, t).
\end{equation}
% In particular, we are interested in
% $$
% (\hat{\boldsymbol\xi}, \hat{\tau}, \hat{t}) = \argmax_{\boldsymbol\xi, \tau, t} \ L_n(\mathbf{y};\boldsymbol\xi, \tau, t),
% $$
% which we call the maximum likelihood values of the parameters $(\boldsymbol\xi, \tau, t)$.
% Since the number of elements in $\boldsymbol\xi$ grows with that of the observed data $\mathbf{y}$,
The typical approach to estimate the tree $\tau$ and branch lengths $t$ involves marginalizing across all possible ancestral states
\begin{equation}
\label{eq:marginal_likelihood}
\tilde{L}_\nCols(\tau, t; \fullAlignment) = \sum_{\fullAncestralStates\in\alphabet^{\nAncestralStateRows\times\nCols}} \ L_\nCols(\tau, t; \fullAlignment, \fullAncestralStates)
\end{equation}
and maximizing over the topology and branch lengths to obtain
$$
(\hat{\tau}, \hat{t}) = \argmax_{\tau, t} \  \tilde{L}_\nCols(\tau, t; \fullAlignment).
$$
% The values $\hat{\boldsymbol\xi}$ are then calculated conditional on these estimates.
% More likely in practice we fix a topology $\tau$ and use this marginalization approach to compute $(\hat{\boldsymbol\xi}, \hat{t})$.

\subsection{Joint maximum likelihood}

An alternative approach does away with the marginalization and directly estimates the maximum likelihood parameters from the fully-observed likelihood in \eqref{eq:full_likelihood}.
One can compute the profile likelihood
\begin{equation}
\label{eq:profile_likelihood}
L_\nCols'(\tau, t; \fullAlignment) = \max_{\fullAncestralStates} \ L_\nCols(\tau, t; \fullAlignment, \fullAncestralStates),
\end{equation}
then estimate the topology and branch lengths via
\begin{equation}
\label{eq:profile_likelihood_topology_bl}
(\hat{\tau}, \hat{t}) = \argmax_{\tau, t} \ L_\nCols'(\tau, t; \fullAlignment),
\end{equation}
using $\hat{\fullAncestralStates}$ maximizing \eqref{eq:profile_likelihood} as an estimate for $\fullAncestralStates$.
Since $\alphabet$ is a discrete set, there exists a maximum \eqref{eq:profile_likelihood}, and \eqref{eq:profile_likelihood_topology_bl} recovers the joint ML values of the unknown parameters.
In general, the functional form of \eqref{eq:profile_likelihood} is determined by inequalities that depend on the unknown $(\tau,t)$.
For this reason, in practice the joint ML strategy estimates $\hat{\fullAncestralStates}$ for a fixed $(\tau,t)$, then $(\hat{\tau},\hat{t})$ given $\hat{\fullAncestralStates}$, maximizing each of these conditional objectives until convergence \cite{Sagulenko2017-jo}.

%EM It seems straightforward to come up with an example of where branch length estimation is inconsistent. It seems like any time a state at an internal node is uncertain we will get different branch lengths by fixing it to the ML state.

\section{Inconsistency of joint inference}

\subsection{Parameter setting}

Consider the Farris topology with arbitrary branch lengths, i.e.,
$$
\tilde{t}=\{x_1,y_1,x_2,y_2,w\}
$$
with edge ordering in the order of the taxa, then the internal branch last.
We now show that, in the case of the Farris tree topology, exchanging $x_1$ with $x_2$ and $y_1$ with $y_2$ does not change the value of the likelihood, and that fixing both the top two branch lengths to be equal and the bottom two branch lengths to be equal will obtain the same maximum likelihood estimate as in the case of arbitrary branch lengths.

\begin{lemma}
For $\tilde{t}=\{x_1,y_1,x_2,y_2,w\}$ and $t=\{x,y,x,y,w\}$,
$$\ell_{\tau_1,t^*}(\tau_1, \tilde{t}; \fullAncestralSplitPartitions(\tau_1,\tilde{t})) = \ell_{\tau_1,t^*}(\tau_1, t; \fullAncestralSplitPartitions(\tau_1,t)).$$
\end{lemma}

Since exchanging $x_1$ and $x_2$ does not change the value of the log likelihood $\ell_{\tau_1,t^*}(\tau_1, \tilde{t}; \fullAncestralSplitPartitions(\tau_1,\tilde{t}))$, $x_1=x_2$ at the maximum.
The analogous statement holds for $y_1$ and $y_2$.
The Felsenstein topology does not admit this property, but, since we are interested in a lower bound for this topology, we simplify the objective function by constraining $x_1=x_2$ and $y_1=y_2$ similarly.

\subsection{Inconsistency in topology estimation}

Define $\ell_{\tau^*,t^*}(\tau, t; \fullAncestralSplitPartitions(\tau,t))$ as
$$
\frac{1}{n}\log L_\nCols'(\tau, t; \fullAlignment) \rightarrow \ell_{\tau^*,t^*}(\tau, t; \fullAncestralSplitPartitions(\tau,t)).
$$
We show the following.
\begin{theorem}
There exist $L_0(x, y)$ and $L_1(x, y)$ such that
$$
\max_{t} \ \ell_{\tau_1,t^*}(\tau_1, t; \fullAncestralSplitPartitions(\tau_1,t)) \le L_0(x, y),
$$
$$
\max_{t} \ \ell_{\tau_1,t^*}(\tau_2, t; \fullAncestralSplitPartitions(\tau_2,t)) \ge L_1(x, y)
$$
and $L_1(x, y) > L_0(x, y)$.
\end{theorem}
Fig.~\ref{fig:inconsistency-farris} shows the region where $L_1(x, y) > L_0(x, y)$.

\begin{figure}
\centering
% obtained by running
% python joint_inf_plot.py --analytic --topology --delta .001 --plot-name figures/topology-inconsistency.svg
\includegraphics[width=.9\textwidth]{topology-inconsistency-inkscape}
\caption{
    Regions of inconsistency.
    Due to the looseness of the upper and lower bounds, the parameters in the top right do not necessarily indicate consistency, though all parameters in the labeled region result in an inconsistency.
}
\label{fig:inconsistency-farris}
\end{figure}

\subsection{Inconsistency in branch length estimation}

We will now consider the problem of branch fidelity estimation on the correct tree using joint ancestral state and branch fidelity estimation.
We will consider two settings on the Farris topology, corresponding to whether some branch fidelities are known ahead of time or not.
As above, we will assume that data is generated with two sister branches of fidelity $x$, with all other branches of length $y$.
We will show that, for a nontrivial subset of possible values for $x$ and $y$, the interior branch fidelity parameter $w'$ will be consistently overestimated as exactly equal to one instead of its true value of $y$.
That is, for the restricted case, with
$$
\hat{w}' = \arg\max_{w'} \ \ell_{\tau_1,\{x,y,y\}}(\tau_1, \{x,y,w'\}; \fullAncestralSplitPartitions(\tau_1,\{x,y,w'\})),
$$
we show there are values for $x$ and $y$ where $\hat{w}'\equiv 1$.
In the general case, we do the same but with
$$
(\hat{x}', \hat{y}', \hat{w}') = \arg\max_{x',y',w'} \ \ell_{\tau_1,\{x,y,y\}}(\tau_1, \{x',y',w'\}; \fullAncestralSplitPartitions(\tau_1,\{x',y',w'\})).
$$
%EM We will show XXX... and introduce \hat{w'} here.
%das: updated

\subsubsection{Restricted case}

First, assume we know the true values of $x'$ and $y'$ but not $w'$.
In other words, we fix $x'=x$ and $y'=y$ to their true, generating values and wish to estimate the internal branch parameter $w'$.
Let
$$
\alpha_1 := \alpha_1(x, y) = \frac{1}{8} (1+x^2+y^2+4xy^2+x^2y^2),
$$
$$
\alpha_2 := \alpha_2(x, y) = \frac{1}{8}(1+x^2+y^2-4xy^2+x^2y^2),
$$
$$
\beta := \beta(x, y) = 1+x^2+y^2+x^2y^2,
$$
$$
\gamma := \gamma(x, y) = 4xy.
$$
\begin{theorem}
\label{thm:restricted-bl}
The maximum likelihood value $\hat{w}' = 1$ if
$$
\beta^2-\gamma^2-2\gamma^2\alpha_1-2\gamma^2\alpha_2+2\gamma\alpha_1\beta-2\gamma\alpha_2\beta \ge 0.
$$
\end{theorem}
The inequality in Theorem~\ref{thm:restricted-bl} is a function of the generating parameters $x$ and $y$.
This region is shown in Fig.~\ref{fig:bl-inconsistency}.

\begin{figure}
\centering
% obtained by running
% python joint_inf_plot.py --analytic --restricted-branch-lengths --delta .001 --plot-name figures/branch-length-inconsistency.svg
\includegraphics[width=.9\textwidth]{branch-length-inconsistency-inkscape}
\caption{Region of inconsistency with $x$ and $y$ fixed.}
\label{fig:bl-inconsistency}
\end{figure}

\subsubsection{General case}

In the general case, $\hat{w}'$ is a function of $x$, $y$, $\hat{x}'$, and $\hat{y}'$.
Looking to the previous section, $\hat{w}'$ is still given by the quadratic formula, only with $\gamma$ and $\beta$ now being functions of $\hat{x}'$ and $\hat{y}'$ instead of $x'$ and $y'$.
Assume we know $\hat{x}'$ and $\hat{y}'$ as functions of $x$ and $y$ only.
\begin{theorem}
\label{thm:general-bl}
For
$$
\gamma_{L}(x, y) \le \gamma(\hat{x}', \hat{y}') \le \gamma_{U}(x, y)
$$
and
$$
\beta_{L}(x, y) \le \beta(\hat{x}', \hat{y}')
$$
then the maximum likelihood value $\hat{w}=1$ when
$$
-\gamma_{U}^2(x, y)(1 + \frac{1}{2}\beta(x, y)) + 2\gamma_{L}(x, y)\beta_{L}(x, y)xy^2 + \beta_{L}^2(x, y) \ge 0.
$$
\end{theorem}
Similar to Theorem~\ref{thm:restricted-bl}, the inequality in Theorem~\ref{thm:general-bl} is also a function of only $x$ and $y$.
The loosest possible bounds are $\gamma_{L} = 0$, $\beta_{L} = 1$ and $\gamma_{U} = 4$, giving
$$
-16(1 + \frac{1}{2}\beta(x, y)) + 1 \ge 0,
$$
which is never true.
The tightest possible bounds are $\gamma_{L} = \gamma_{U} = \gamma(x, y)$ and $\beta_{L} = \beta(x, y)$, which is the case in Figure~\ref{fig:bl-inconsistency}.
As an example middle case, let the error in estimating both $\hat{x}'$ and $\hat{y}'$ be $.1$ in either direction, i.e., $x-.1\le\hat{x}'\le x+.1$ and $y-.1\le\hat{y}'\le y+.1$.
In this case, the region to the left side of the curve in Figure~\ref{fig:bl-loose-inconsistency} will either estimate $\hat{w}'$ to be one or estimate either $\hat{x}'$ or $\hat{y}'$ to be more than $.1$ away from its true value.

\begin{figure}
\centering
% obtained by running
% python joint_inf_plot.py --analytic --general-branch-lengths --delta .001 --plot-name figures/bl-loose-inconsistency.svg
\includegraphics[width=.9\textwidth]{bl-loose-inconsistency-inkscape}
\caption{Region of inconsistency with $x-.1 \le \hat{x}' \le x+.1$ and $y-.1 \le \hat{y}' \le y+.1$.}
\label{fig:bl-loose-inconsistency}
\end{figure}

\subsubsection{Empirical validation}

The region of inconsistency in Figure~\ref{fig:bl-loose-inconsistency} is potentially a conservative estimate of where $\hat{w}=1$.
To determine how conservative, we use the method of differential evolution \cite{Storn1997} to perform joint maximum likelihood estimation.
Since empirical estimates can be somewhat unstable, we perform two maximizations---one over $0 \le x',y',w' \le 1$ and one over $0 \le x',y' \le 1$ with $w'=1$---and take the value of $\hat{w}'$ with the higher likelihood.
We compute these maxima over a lattice in steps of $10^{-3}$ for $x$ and $y$ from $10^{-3}$ to $1-10^{-3}$---we do not include zero or one in our lattice to further stabilize the fits, recalling from Section~\ref{sec:restricted-case} that cases where $x$ or $y$ are equal to zero or one can result in pathologies.
From this process we obtain Fig.~\ref{fig:bl-general-inconsistency}.

The estimates for $\hat{w}'$ when $x$ and $y$ are both large seem to be unbiased, with $\hat{w}'$ increasing as $y$ increases, though the region where $\hat{w}'$ is estimated to be one---indicating no change along that particular branch---is similar to that in the restricted case.
Although the restricted and general case are similar, comparing Figures~\ref{fig:bl-inconsistency} and~\ref{fig:bl-general-inconsistency} shows that the region of inconsistency in the general case has a slightly different character, possibly due to potential looseness of the bound in the restricted case.
%das: TODO double check this; do the same empirical plot for the restricted case
This region encompasses the majority of the parameter space; even given the correct Farris topology and performing our best possible optimization, we have a region of the space of true parameters where we will always estimate the interior branch parameter to be one.

\begin{figure}
\centering
% obtained by running
% python joint_inf_plot.py --empirical --general-branch-lengths --delta .001 --plot-name figures/w-hat-empirical-001.svg --pkl-name figures/w-hat-empirical-001.pkl --n-jobs 16
% or
% python joint_inf_plot.py --empirical --general-branch-lengths --plot-name figures/w-hat-empirical-001.svg --in-pkl-name figures/w-hat-empirical-001.pkl
% if fit already
\includegraphics[width=.9\textwidth]{w-hat-empirical-001}
\caption{
    %das: note to update caption
    Region where $\hat{w}'=1$ computing empirical estimates of $(\hat{x}', \hat{y}', \hat{w}')$.
}
\label{fig:bl-general-inconsistency}
\end{figure}

\section{Discussion}

Neyman-Scott paradox.

Interesting that here we are simulating on the Farris tree and end up with the Felsenstein tree.
For maximum parsimony it's the opposite \cite{Felsenstein1978-rr}.
%EM We should be clear here that thus this is not "long branch attraction". I haven't read the Farris papers, but I wonder what his argument is and if it relates to this, in that there are too many parameters. Probably something should go in the intro as well pointing out that this is not LBA.
%das: Noted. I will work on this.
Discussion of number of parameters of each.

However, note that \cite{Siddall1998-hq} get things going the same way, although \cite{Swofford2001-hr} show that the problem is that they didn't simulate long enough sequences.

\bibliographystyle{plain}
\bibliography{joint_inf}

\newpage

\input{appendix}

\end{document}
